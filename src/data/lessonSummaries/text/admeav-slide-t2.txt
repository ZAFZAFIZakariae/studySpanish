Advanced methods of artificial vision
Unit 2: CNN-based feature extraction

Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
2

Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
3

4
A neural network (NN) is a set of layers composed of neurons.
NN of 2 layers NN of 3 layers
fully connected layers: Denses
Neural Networks: Structure

5
𝑓෍
𝑖
𝑤𝑖𝑥𝑖 +𝑏𝑤1𝑥1
𝑤2𝑥2
𝑓 𝑧 = 𝑎
𝒛
𝑥1
𝑥2
1 1
𝑤1,1
0
𝑤2,1
0
𝑤1,2
0
𝑤2,2
0
𝑤1,1
1
𝑤2,1
1
𝑏0 𝑏1
𝑧1
1
𝑧2
1
𝑎1
1
𝑎2
1
𝑧1
2 𝑎1
2 ො𝑦
Neural Networks: Structure

6
ෝ𝑦𝑖
𝑤1,1
0
𝑤1,2
0
𝑤1,1
1
𝑤4,4
1
𝑓(𝑊1𝑥𝑖 +𝑏0)
𝑊0 =
𝑤1,1
0 𝑤2,1
0 𝑤3,1
0
𝑤1,2
0 𝑤2,2
0 𝑤3,2
0
𝑤1,3
0
𝑤1,4
0
𝑤2,3
0
𝑤2,4
0
𝑤3,3
0
𝑤3,4
0
𝐷1×𝐷
𝑊1 = 𝐷2×𝐷1
𝑊2 = 𝐾×𝐷2
𝑓(𝑊1𝑎1 +𝑏1) 𝑓(𝑊2𝑎2 +𝑏2)
𝑥𝑖 =
𝑥1
𝑥2
𝑥3
 
𝑤1,1
2
Neural Networks: Structure

Optimization
7
MLP   𝑥𝑖
ො𝑦𝑖
𝐽 = ෍
𝑖
𝑦𝑖 − ො𝑦𝑖 2
𝑦𝑖
Optimise 𝑊0, 𝑊1, 𝑊2, 𝑏0, 𝑏1, 𝑏2

Optimization
8
𝐽 = ෍
𝑖
𝑦𝑖 − ො𝑦𝑖 2 𝑎𝑟𝑔𝑚𝑖𝑛𝑊,𝑏 𝐽
𝛻𝐽 = 0 𝛻𝐽 = 𝜕𝐽
𝜕𝑤0
𝜕𝐽
𝜕𝑤1
= 0
𝐽 = 2+0.5× 𝑤0
2 +𝑤1
2 −0.5×𝑤0 ×𝑤1 +1.7×𝑤1
To solve this system

Optimization: Gradient Descent
9
Iterative process:
𝑤0(𝑡+1) = 𝑤0(𝑡)−η 𝜕𝐽
𝜕𝑤0
𝑤1(𝑡+1) = 𝑤1(𝑡)−η 𝜕𝐽
𝜕𝑤1
η: learning rate
- High values: fast learning but be 
careful with convergence
- Valores bajos: slow learning

Gradient descent variants
10
• Gradient Descent: original (vanilla version)
• For each training set data:
• Fordward pass
• Error calculation and accumulation
• Update the weights (backpropagation)
• Stocasthic Gradient Descent (SGD)
• For each training set data:
• Fordward pass
• Error calcutlation and accumulation
• Update the weights (backpropagation)
• Minibacth Gradient Descent (SGD)
• Divide the training set in batches
• For each batch
• For each data from the batch
• Fordward pass
• Error calculation and accumulation
• Update the weights (backpropagation)
The weights are updated 
only once in the training set
The weights are updated for 
each data in the training set
The weights are updated for 
each batch in the training 
set
Consume a lot of 
resources but it is 
more precise
Optimal with 
vectorised
versions of 
the 
algorithm
Hyperparameter to optimise, the batch size: for example in CNNs it is 
usually 256 for a training set of 1.2 million.

Forward-Backward propagation
11
Training data

Effect of learning rate
12

Data partitioning
13
data
Training (70-80%) Test (20-30%)
20-30%  training
Validation Test (20-30%)Training
batch1 batch2 batch3 batch4 batch5 .  .  . Batch N-1 batch N

Training process
14
. . .
epoch 1 epoch M
Training Training Training
Validation Validation Validation
Model 1 Model 2 Model M
Test
Model with highest accuracy 
or lowest validation loss
epoch 2

Activation functions
15
Sigmoid
Hyperbolic tangent
Relu (Rectified Linear Unit)
𝑓 𝑥 = 𝜎 𝑥 = 1
1+𝑒−𝑥
𝑓 𝑥 = 𝑡𝑎𝑛ℎ 𝑥 =2𝜎 2𝑥 −1
𝑓 𝑥 = max(0,𝑥)
𝑓 𝑥 = α∗𝑥 𝑥 < 0 +𝑥(𝑥 ≥ 0)
Elu (Exponential Linear Unit) (𝛼=1) 
Leaky Relu (𝛼=0.3) 
𝑓 𝑥 = α∗(𝑒𝑥 −1) 𝑥 < 0 +𝑥(𝑥 ≥ 0)

Optimizers
16

17

18
Vary the learning rate throughout the training, with epochs: larger at the 
beginning and more precise at the end.
•  “Step decay”: Reduce the learning rate by some factor every few 
epochs.Typical values might be to reduce the learning rate by half every 5 
epochs, or by 0.1 every 20 epochs. 
In practice: (observe the validation error while training with a fixed learning rate, 
and reduce the learning rate by a constant (e.g. 0.5) every time the validation 
error stops improving.
•  “Exponential decay”: 
𝜂 = 𝜂0𝑒−𝑘𝑡
• "1/t decay”: 
𝜂 = 𝜂0
1+𝑘𝑡𝑒−𝑘𝑡
𝜂0,𝑘: hyperparameters    𝑡: number of epoch
NN: learning rate optimization

Detecting overfitting
19

Detecting overfitting
20

21
Regularization penalty
𝝀 :regularization strength
– 𝑳𝟐
– 𝑳𝟏
– Elastic Net
𝐿 = 1
𝑁෍
𝑖=1
𝑁
𝐿𝑖 𝐿 = 1
𝑁෍
𝑖=1
𝑁
𝐿𝑖 +λ𝑅(𝑊)
𝑅(𝑊) = ෍
𝑖
෍
𝑗
𝑊𝑖,𝑗
2
𝑅(𝑊) = ෍
𝑖
෍
𝑗
𝑊𝑖,𝑗
𝑅(𝑊) = ෍
𝑖
෍
𝑗
𝑊𝑖,𝑗
2 +𝛽 𝑊𝑖,𝑗 W(𝑡 +1) = 𝑊(𝑡)−η𝛻𝐽
W 𝑡 +1 = 𝑊 𝑡 −η𝛻𝐽+λ𝑅(𝑊)
Update without regularization
Update with regularization
Loss with regularizationLoss without regularization

Drop-oupt
22
•  “drop-out”: randomly disconnect inputs from one layer to the next with a probability 𝑝: prob. to maintain 
the link or to disconnect it (implementation dependent)
• For a batch, the links are deactivated for the forward and backward propagation stages, then reactivated and 
randomly deactivated again for the next batch.
• dropout only is used in training.
•  Disconnecting connections randomly ensures that no single node is always responsible for "activating" when a 
given pattern occurs. It ensures that there are multiple, redundant nodes that will be triggered when similar inputs 
are presented, which in turn helps our model to generalise.
Without drop-out Drop-out with 𝑝 = 0.5

23
KERAS
• Keras is a high-level framework for training neural networks. This library was 
developed by François Chollet in 2015 with the aim of simplifying the programming 
of algorithms based on deep learning.
• It offers a more intuitive and high-level set of abstractions. Training can still be 
done on GPU, remembering that this is the only way we have to train a neural 
network in an allowable time interval.

24
Basic layers in Keras
keras.layers.Dense(units, activation=None, use_bias=True, 
kernel_initializer='glorot_uniform', bias_initializer='zeros')
keras.layers.Activation(activation)
keras.layers.Dropout(rate, noise_shape=None, seed=None)
keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid')
keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', 
data_format=None)
keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, 
center=True, scale=True)
keras.layers.Flatten(data_format=None)
https://keras.io/layers/core/ 
Keras documentation

25
Architecture definition: 
Sequential mode
Sequential mode (or API): An object of type Model () is instantiated and the layers 
that make up the architecture are added one after the other.
# Imports 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
# Sequential model
model = Sequential()
# Architecture definition
model.add(Dense(64, input_dim=784))
model.add(Activation('relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))
https://keras.io/models/sequential/

26
Architecture definition: 
Functional mode
Functional mode (or API): An input is defined and the architecture is defined from these inputs (indicating which 
is the input to each layer). Once the architecture has been defined, the model object is created by passing it the 
inputs and outputs (last layer defined).
# Imports 
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model 
# Input definition
inputs = Input(shape=(784,))
# Architecture definition
x = Dense(64, activation='relu')(inputs)
x = Dense(64, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)
# We create the model object as a union of inputs and architecture
model = Model(inputs=inputs, outputs=predictions)
https://keras.io/models/model/

27
Compiling a Keras model
Before training the model in Keras, it is necessary to compile it by configuring the training 
process. This process is carried out by means of the command model.compile.
# Example for a multi-class classification problem
model.compile(optimizer=‘sgd',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# Example for a binary classification problem
model.compile(optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, 
nesterov=True),
              loss='binary_crossentropy',
              metrics=['accuracy'])
# Example for a regression problem
model.compile(optimizer='rmsprop',
              loss='mse')

28
Training in Keras
Once the model has been compiled in Keras it is possible to launch the training process. Keras trains models 
from data and labels stored in Numpy arrays using the model.fit method.
# Data generation
import numpy as np
data = np.random.random((1000, 100))
labels = np.random.randint(2, size=(1000, 1)) 
# Example with labels in one-hot encoding
model.compile(optimizer='rmsprop',loss='categorical_crossentropy',
              metrics=['accuracy'])
one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)
model.fit(data, one_hot_labels, epochs=10, batch_size=32)
# Example with categorical labels
model.compile(optimizer='rmsprop', loss=‘sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(data, labels, epochs=10, validation_data=(data_v, labels_v), 
batch_size=32)

29
Prediction and evaluation in 
Keras
Once the trained model is available, an evaluation of the model must be carried out. T o do 
this, the test data is predicted with the model.predict command and then performance 
metrics are obtained or model.evaluate is used
# Example with model.evaluate
model.evaluate(x=X_test, y=y_test, batch_size=None)
%% Returns the values of losses and metrics used in training for the test data.
# Example with model.predict and evaluation_report
from sklearn.metrics import classification_report
predictions = model.predict(x=X_test, batch_size=None)
print(classification_report(y_labels,predictions.argmax(axis=1))
%% Returns more metrics

Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
30

Concept of image
31
Gray image RGB image
level: 0….255 (black….white)

Problem with images
32
28
28
784
200
200x784= 156.800 weights
Example: In the MNIST database for image classification
• When the size of the image starts to grow, using fully connected 
layers (one neuron per pixel) becomes unfeasible.

Hand-crafted learning
33
Hand-crafted learning
HOG
LBP
Co-occurrence
Granulometry SIFT
SURF
Colour
Data Feature Extraction Classification or 
Regression

Automated learning
34
(Data)2
Feature extraction Classification
Deep neural networks

Convolutional neural networks
35
35
• Convolutional networks are a specialized kind of neural network for 
processing data that has a grid-like topology. They take advantadge of:
– local connectivity
– parameter sharing
– pooling / subsampling hidden units
Multilayer Perceptron Convolutional neural network
3

Convolution
36
74 66 62 59 63 71 75 72
72 65 61 58 62 70 75 72
71 68 63 59 63 69 73 69
70 67 62 59 62 69 73 69
70 64 59 57 61 70 74 72
68 66 64 63 65 68 69 68
70 69 67 66 67 69 69 67
69 68 66 64 65 67 66 63
1 0 -1
2 0 -2
1 0 -1
-1 0 1
-2 0 2
-1 0 1
𝑦 𝑚,𝑛 = 𝑥 𝑚,𝑛 ∗ℎ 𝑚,𝑛 = ෍
𝑘
෍
𝑙
𝑥 𝑘,𝑙 ℎ[𝑚−𝑘,𝑛−𝑙]
Kernel
ℎ[−𝑘,−𝑙]ℎ[𝑘,𝑙]
−1 ∗62+0∗59+1∗62+(−2)∗59+0∗57+2∗61+(−1)∗64+0∗63+1∗65 = 3
3
Imagen de salida

37
Convolution
1 0 -1
2 0 -2
1 0 -1
1 2 1
0 0 0
-1 -2 -1

Local connectivity
38
38
 Receptive field
෍ 𝑓
 ෍ 𝑓
 ෍ 𝑓
… …
each hidden unit is connected only to a subregion (patch) 
of the input image: receptive ﬁeld
w111 … w1N1
… … …
wN11 … wNN1
w111 … w1N1
… … …
wN11 … wNN1

Parameter sharing
39
39
෍ 𝑓
 ෍ 𝑓
 ෍ 𝑓
… …
 ෍ 𝑓
 ෍ 𝑓
 ෍ 𝑓… …
 ෍ 𝑓
 ෍ 𝑓
 ෍ 𝑓… …
units organized into the same
‘‘feature map’’ share parameters
Feature map 1 Feature map 2 Feature map 3
hidden units within a feature
map cover diﬀerent positions in
the image
w111 … w1N1
… … …
wN11 … wNN1
w111 … w1N1
… … …
wN11 … wNN1
w111 … w1N1
… … …
wN11 … wNN1
Kernel of 
feature 
map 1
Kernel of 
feature map 2
Kernel of 
feature map 3

Feature maps
40
Input image
Feature maps
Convolutions
w111 … w1N1
… … …
wN11 … wNN1
Learnable

Feature maps
41
Input image Feature map 1 Feature map 2

Pooling
42
maxpooling
Pooling is performed in non overlapping neighborhoods (subsampling)

Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
43

Convolutional layers
44
Filtro

Convolutional layers
45
Presentación
N. of input images 
𝑁𝑖𝑛𝑝𝑢𝑡
Image 
size
Input images 
(from the 
previous layer)
Output images 
(to the next layer)
Kernels 
(𝐾 ×𝐾 ×𝑁𝑖𝑛𝑝𝑢𝑡 ×𝑁𝑜𝑢𝑡𝑝𝑢𝑡)
N. of output images 
𝑁𝑜𝑢𝑝𝑢𝑡
𝐾
𝐾
𝑁𝑖𝑛𝑝𝑢𝑡
𝑁𝑜𝑢𝑡𝑝𝑢𝑡

Convolutional layers: Stride
Input 
image
7 x 7
Activation map
5 x 5
Stride = 1
Kernel = 3 x 3
• The center of the kernel (or mask filter) moves to the next pixel in steps given by 
the Stride hyperparameter
Warning !!! 
Reduction of the image size.
It depends on kernel size and stride46

Convolutional layers: zero 
padding
47
What happens when a 5 x 5 x 3 filter is applied to a 32 x 32 x 3 input volume (32 x 32 
pixel RGB image)? 
In the first layers of the network we want to preserve as much information as possible 
from the original image.
Size of activation map: 28 x 28 x 3
Zero
Padding
5 x 5 x  3
Kernel
Ouptut image
28 x 28 x 3
Input image
32 x 32 x 3
Zero padding
36 x 36 x 3
Input image
32 x 32 x 3
5x5x3
Kernel
Ouptut image
28 x 28 x 3

Pooling layers
48
• This layer is a decimation layer in spatial dimension (width, height)
• Its function reduces the spatial size of the representation :
• Reduce the number of paremeters
• Reduce computational cost
• Avoid overfitting
Main functions:
- Max pooling
- Average pooling
- L2-norm pooling
Typical hyperparameters:
• Filter size = 2×2
• Stride = 2
Input volume Output volume
Stride=2
5 8
9 6
3 5
7 3
Avg.

CNN. Setup
• Among the different layers, some aditional parameters have to been 
considered :
– Convolutional layer
• Kernel size
• Number of filters
• Stride 
• Zero padding
– Pooling layer
• Stride 
• Window size
– Activation layer
– Batch normalization layer
– Drop out layer
– Classification: Top model
49

50
Basic layers in Keras
keras.layers.Dense(units, activation=None, use_bias=True, 
kernel_initializer='glorot_uniform', bias_initializer='zeros')
keras.layers.Activation(activation)
keras.layers.Dropout(rate, noise_shape=None, seed=None)
keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid')
keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', 
data_format=None)
keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, 
center=True, scale=True)
keras.layers.Flatten(data_format=None)
https://keras.io/layers/core/ 
Keras documentation

Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
51

CNN: Feature extraction
52
Feature visualization of convolutional net trained on ImageNet (Zeiler, Fergus, 2013)
• CNN = learning hierarchical representations with increasing levels of abstraction
• End to end training: joint optimization of features and classiﬁer
52

53
Feature extraction
http://cs231n.stanford.edu/53

Low-level features
54
• How does the network detect low-level features?
• Convolutional blocks: The output (activation
map) of the first convolutional layer will be the
input of the second layer,and so forth.
•The output of the second convolutional layer
will be the activations representing higher
level features: semicircles (combination of a
curve and a straight edge), squares
(combination of several straight edges), etc.
54

High-level features
55
55

High-level features
56
56

High-level features
57
57 https://poloclub.github.io/cnn-explainer/

Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
58

CNN for classification
• Description of CNN
59
https://miro.medium.com/v2/resize:fit:847/1*eiWpS
Vh65QZN9usE6YRJTA.png
• Training: stochastic gradient descent/ forward-backward propagation
• Hyperparameters setting: as MLP
59

CNN for classification
60
Convolutional 
layers
Feature extraction Classification
• Top model:  
• Flatten layer
• Multilayer perceptrón
• The last fully connected layer is a softmax layer (in the case of more than one output 
neuron) with as many neurons as the number of classes.
• PROBLEM: Overfitting

CNN for classification
61
• Top model:  
• Global averagepooling + softmax
• Global maxpooling+ softmax
Convolutional 
layers
Feature extraction
Global average 
pooling
Global max 
pooling
Classification
GAP: calculates the average 
output of each feature map
GMP: calculates the maximum 
output of each feature map

CNN.  Final Global Architecture
62

Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
63

ImageNet
• ImageNet is a database containing 14 million images classified into 1000 
different categories (http://www.image-net.org/).
• The most popular state-of-the-art architectures have been trained with ImageNet and 
the resulting network weights are publicly available. 
• The ImageNet Project runs an annual competition, the ImageNet Large Scale Visual 
Recognition Challenge (ILSVRC), where algorithms compete to correctly classify 
objects and scenes. 
64
Objetive: to reduce training
parameters while maintaining or
exceeding the previous year
results, with a strong focus on
reducing computational cost.

LeNet-5
65
• LeNet-5 (1998, Lecun et al.): CNN pioneer.
• 7 layers 
• Aplication: digit classification (banks to recognise
handwritten digits on cheques).
• Graylevel images of 28x28 pixels. Higher resolution 
images: more convolutional layers.
Convolution Convolution
Convolution
Pooling
Pooling
Fully 
Connected

ALexNet
66
•  Winner of the 2012 ImageNet challenge and significantly outperformed all competitors by reducing the 
top-5 error from 26% (runner-up) to 15.3%. SuperVision group.
•  Deeper than LeNet and with more filters per layer.
o 11x11, 5x5,3x3 convolutional filters and RELU activations.
o Max pooling layers
o Dropout
o Data augmentation 
o SGD with momentum
o Trained for 6 days using 2 Nvidia Geforce GTX 580 GPUs.

VGG16 y VGG19
67
•  Fairly simple architecture: blocks composed of an incremental number of convolutional
layers with filters of size 3x3 with interleaved maxpooling layers (halving the size of the
activation maps).
• Classification block: 2 fully-connected layers (4096 neurons) and the output layer (1000
neurons).
• 16 and 19: differences in number of weighted layers in each network (convolutional and fully
connected).
Simonyan y Zisserman (2014). Very Deep 
Convolutional Networks for Large Scale 
Image recognition 
(https://arxiv.org/abs/1409.1556)

Inception V3 (GoogleNet)
68
•  This type of architecture, introduced in 2014 by Szegedy et al. "Going Deeper with
Convolutions" (https://arxiv.org/abs/1409.4842), uses blocks with filters of different sizes that are
then concatenated in order to extract features at different scales that are then combined into a
single activation map.
• Challenge winners in 2014, top-5 error rate of 6.67%!
• This architecture, 22 convolutional layers, requires less memory than VGG and ResNet. It
reduces the number of parameters from 60 million (AlexNet) to 4 million.

Xception
69
• Proposed by François Chollet (creator of Keras).  Like Inception but provides a quick way to 
make 2D convolutions (2 1D convolutions).
"Xception: Deep Learning with Depthwise Separable Convolutions", https://arxiv.org/abs/1610.02357.

ResNet (Microsoft)
70
•  Developed by He et al. in 2015 ( 2015 winners): introduces an exotic type of 
architecture based on modules (" networks within networks "). 
• Introduces the concept of " residual connections ". In layer 𝑙+1 the activation 
map of the unmodified and modified 𝑙-layer 𝑙+1 is combined.. 
“Deep Residual Learning for Image Recognition": https://arxiv.org/abs/1512.03385

ResNet
71
•  Improved architecture in 2016 improved by including more layers of residual blocks.
• There are variations of ResNet with different numbers of layers, but the most widely used is
ResNet50, which consists of 50 with weights.
• Many more layers than VGG but needs almost 5 times less memory: fully connected layers are
replaced by a type of layer called GlobalAveragePooling, which converts 2D activation maps
to a vector of n classes that is used to calculate the probability of belonging to each class.

Comparison of sizes
72
Source: AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS, 2017

73
Comparison: accuracy vs number of 
parameters
Source: AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS, 2017

Transfer learning
•  Training a neural network is costly (time). 
• Transfer learning techniques to avoid:
o Define the architecture of a neural network.
o Train it from the beginning. 
• Idea: Initialise the weights of a predefined network with values that classify a given dataset 
well and tune them to our problem.  
• We avoid:
o Need for a dataset as large as necessary if we want to train a network from scratch (from hundreds 
of thousands or even millions of images we could go to a few thousand). 
o Need to wait a good number of epochs to get values for the optimal weights for classification.
•  Two techniques:
– Transfer learning 
– Fine-tuning
74

Transfer Learning vs Fine-
Tuning
75

Transfer Learning
76
• SVM
• Random Forest
• etc.
Extractor de caracterísiticas Clasificado
r

Fine-tuning
77
Shallow-tuning

Fine-tuning
78
Deep-tuning

Fine-tuning
79
Deep-tuning

Fine-tuning
80
From 
scratch

Practical Protocol
81
In practise:
1. Modify only the last layer to have the same number of outputs as 
classes(baseline).
2. Modify the top model and re-train the classifying stage (FC 
layers) (“shallow tuning”).
3. Re-train convolutional blocks (“deep tuning”). In each iteration 
one more, starting from the output.
Tips depending on our dataset 
o If it is small and similar to the original: transfer learning (SVM for 
example). It helps to prevent overfitting. 
o If it is large and similar to the original: as we have more data we 
probably won't incur in over-fitting, so we can do fine-tuning with 
more confidence.

Practical protocol
82
Tips depending on our dataset 
o If it is small and very different from the original: transfer learning 
but with characteristics of layers before the last convolutional. 
o If it is large and very different from the original: train the network 
from scratch. (from scratch). 
Note the possible restrictions of pre-trained
models. For example, they may require a
minimum image size. In addition, when re-
training networks, learning rates are usually
chosen lower than if we do it from scratch,
since we start from an initialisation of
weights that is assumed to be good.

Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
83

84
Image DataGenerator class
• Keras has implemented functionalities to facilitate/optimise data loading.
• It is possible to load the training data in RAM in batches (batch by batch) to perform the steps
that make up a training step and not having to store the entire dataset in memory.
• ImageDataGenerator is nothing more than an object type that can apply or not certain
transformations to the data being loaded by means of a series of methods of this object that
facilitate the task.
• The ImageDataGenerator object does not store the data itself.
• The transformations that ImageDataGenerator allows serve to preprocess the data, rescale it or
establish a validation partition, but its main function is that it implements the functionality to
create synthetic image samples.
https://keras.io/api/preprocessing/image/#imagedatagenerator-class

85
Loading batches of data from 
disk
The ImageDataGenerator object includes a series of methods that facilitate the task of loading data
from disk. These methods allow the data to be loaded gradually into memory (batch by batch)
during the training process.
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator

86
Loading batches of data 
from disk
The ImageDataGenerator object includes a series of methods that facilitate the task of loading data
from disk. These methods allow the data to be loaded gradually into memory (batch by batch)
during the training process.
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator

87
•  Objetive: Increase the number of images in the training set to combat overfitting.
•  How?
o Rotation,
o Re-scale,
o Translation,
o Zoom,
o Flips (Horizontal and Vertical),
o Changes in color.
• This is done in real time, during training. It is not necessary to save the augmented
images.
• The new images take the class of the source image, so the transformations cannot
be too exaggerated, let alone cause them to resemble another class.
Data Augmentation

Early Stopping
88
https://keras.io/api/callbacks/early_stopping/

89
Data augmentation
# Creating an instance of ImageDataGenerator class 
train_datagen = ImageDataGenerator(rescale=1. / 255,                                   
                 rotation_range = 0.2,
                                   zoom_range = 0.2,
        horizontal_flip = True,
                                   vertical_flip = True)
# Creating an instance of ImageDataGenerator class 
validation_generator = validation_datagen.flow_from_directory(validationDirectory,
                                                      target_size=(img_width, img_height),
                                                      batch_size=batch_size,
                                                      class_mode='categorical’)
Source: https://medium.com/towards-data-science/image-augmentation-for-deep-learning-
using-keras-and-histogram-equalization-9329f6ae5085 )

Reduce learning rate
90
https://keras.io/api/callbacks/learning_rate_scheduler/
Step decay
Exponential decay
Cosine Annealing

Model checkpoint
91
https://keras.io/api/callbacks/model_checkpoint/