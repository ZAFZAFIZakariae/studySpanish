# Extracted content
Source: subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_.pdf

### Page 1
Advanced methods of artificial vision
Unit 2: CNN-based feature extraction
![Page 1, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_001.png)
![Page 1, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_001_img_001.png)
![Page 1, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_001_img_002.png)

### Page 2
Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
2
![Page 2, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_002.png)
![Page 2, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_002_img_002.png)

### Page 3
Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
3
![Page 3, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_003.png)

### Page 4
4
A neural network (NN) is a set of layers composed of neurons.
NN of 2 layers NN of 3 layers
fully connected layers: Denses
Neural Networks: Structure
![Page 4, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_004.png)

### Page 5
5
𝑓෍
𝑖
𝑤𝑖𝑥𝑖 +𝑏𝑤1𝑥1
𝑤2𝑥2
𝑓 𝑧 = 𝑎
𝒛
𝑥1
𝑥2
1 1
𝑤1,1
0
𝑤2,1
0
𝑤1,2
0
𝑤2,2
0
𝑤1,1
1
𝑤2,1
1
𝑏0 𝑏1
𝑧1
1
𝑧2
1
𝑎1
1
𝑎2
1
𝑧1
2 𝑎1
2 ො𝑦
Neural Networks: Structure
![Page 5, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_005.png)

### Page 6
6
ෝ𝑦𝑖
𝑤1,1
0
𝑤1,2
0
𝑤1,1
1
𝑤4,4
1
𝑓(𝑊1𝑥𝑖 +𝑏0)
𝑊0 =
𝑤1,1
0 𝑤2,1
0 𝑤3,1
0
𝑤1,2
0 𝑤2,2
0 𝑤3,2
0
𝑤1,3
0
𝑤1,4
0
𝑤2,3
0
𝑤2,4
0
𝑤3,3
0
𝑤3,4
0
𝐷1×𝐷
𝑊1 = 𝐷2×𝐷1
𝑊2 = 𝐾×𝐷2
𝑓(𝑊1𝑎1 +𝑏1) 𝑓(𝑊2𝑎2 +𝑏2)
𝑥𝑖 =
𝑥1
𝑥2
𝑥3

𝑤1,1
2
Neural Networks: Structure
![Page 6, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_006.png)

### Page 7
Optimization
7
MLP   𝑥𝑖
ො𝑦𝑖
𝐽 = ෍
𝑖
𝑦𝑖 − ො𝑦𝑖 2
𝑦𝑖
Optimise 𝑊0, 𝑊1, 𝑊2, 𝑏0, 𝑏1, 𝑏2
![Page 7, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_007.png)
![Page 7, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_007_img_003.png)

### Page 8
Optimization
8
𝐽 = ෍
𝑖
𝑦𝑖 − ො𝑦𝑖 2 𝑎𝑟𝑔𝑚𝑖𝑛𝑊,𝑏 𝐽
𝛻𝐽 = 0 𝛻𝐽 = 𝜕𝐽
𝜕𝑤0
𝜕𝐽
𝜕𝑤1
= 0
𝐽 = 2+0.5× 𝑤0
2 +𝑤1
2 −0.5×𝑤0 ×𝑤1 +1.7×𝑤1
To solve this system
![Page 8, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_008.png)

### Page 9
Optimization: Gradient Descent
9
Iterative process:
𝑤0(𝑡+1) = 𝑤0(𝑡)−η 𝜕𝐽
𝜕𝑤0
𝑤1(𝑡+1) = 𝑤1(𝑡)−η 𝜕𝐽
𝜕𝑤1
η: learning rate
- High values: fast learning but be
careful with convergence
- Valores bajos: slow learning
![Page 9, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_009.png)

### Page 10
Gradient descent variants
10
• Gradient Descent: original (vanilla version)
• For each training set data:
• Fordward pass
• Error calculation and accumulation
• Update the weights (backpropagation)
• Stocasthic Gradient Descent (SGD)
• For each training set data:
• Fordward pass
• Error calcutlation and accumulation
• Update the weights (backpropagation)
• Minibacth Gradient Descent (SGD)
• Divide the training set in batches
• For each batch
• For each data from the batch
• Fordward pass
• Error calculation and accumulation
• Update the weights (backpropagation)
The weights are updated
only once in the training set
The weights are updated for
each data in the training set
The weights are updated for
each batch in the training
set
Consume a lot of
resources but it is
more precise
Optimal with
vectorised
versions of
the
algorithm
Hyperparameter to optimise, the batch size: for example in CNNs it is
usually 256 for a training set of 1.2 million.
![Page 10, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_010.png)

### Page 11
Forward-Backward propagation
11
Training data
![Page 11, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_011.png)
![Page 11, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_011_img_003.png)
![Page 11, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_011_img_004.png)

### Page 12
Effect of learning rate
12
![Page 12, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_012.png)

### Page 13
Data partitioning
13
data
Training (70-80%) Test (20-30%)
20-30%  training
Validation Test (20-30%)Training
batch1 batch2 batch3 batch4 batch5 .  .  . Batch N-1 batch N
![Page 13, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_013.png)

### Page 14
Training process
14
. . .
epoch 1 epoch M
Training Training Training
Validation Validation Validation
Model 1 Model 2 Model M
Test
Model with highest accuracy
or lowest validation loss
epoch 2
![Page 14, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_014.png)
![Page 14, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_014_img_007.png)

### Page 15
Activation functions
15
Sigmoid
Hyperbolic tangent
Relu (Rectified Linear Unit)
𝑓 𝑥 = 𝜎 𝑥 = 1
1+𝑒−𝑥
𝑓 𝑥 = 𝑡𝑎𝑛ℎ 𝑥 =2𝜎 2𝑥 −1
𝑓 𝑥 = max(0,𝑥)
𝑓 𝑥 = α∗𝑥 𝑥 < 0 +𝑥(𝑥 ≥ 0)
Elu (Exponential Linear Unit) (𝛼=1)
Leaky Relu (𝛼=0.3)
𝑓 𝑥 = α∗(𝑒𝑥 −1) 𝑥 < 0 +𝑥(𝑥 ≥ 0)
![Page 15, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_015.png)
![Page 15, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_015_img_003.png)
![Page 15, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_015_img_004.png)
![Page 15, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_015_img_005.png)
![Page 15, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_015_img_006.png)
![Page 15, Figure 6](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_015_img_007.png)

### Page 16
Optimizers
16
![Page 16, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_016.png)
![Page 16, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_016_img_003.png)
![Page 16, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_016_img_004.png)

### Page 17
17
![Page 17, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_017.png)
![Page 17, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_017_img_003.png)
![Page 17, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_017_img_004.png)

### Page 18
18
Vary the learning rate throughout the training, with epochs: larger at the
beginning and more precise at the end.
•  “Step decay”: Reduce the learning rate by some factor every few
epochs.Typical values might be to reduce the learning rate by half every 5
epochs, or by 0.1 every 20 epochs.
In practice: (observe the validation error while training with a fixed learning rate,
and reduce the learning rate by a constant (e.g. 0.5) every time the validation
error stops improving.
•  “Exponential decay”:
𝜂 = 𝜂0𝑒−𝑘𝑡
• "1/t decay”:
𝜂 = 𝜂0
1+𝑘𝑡𝑒−𝑘𝑡
𝜂0,𝑘: hyperparameters    𝑡: number of epoch
NN: learning rate optimization
![Page 18, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_018.png)

### Page 19
Detecting overfitting
19
![Page 19, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_019.png)

### Page 20
Detecting overfitting
20
![Page 20, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_020.png)
![Page 20, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_020_img_003.png)

### Page 21
21
Regularization penalty
𝝀 :regularization strength
– 𝑳𝟐
– 𝑳𝟏
– Elastic Net
𝐿 = 1
𝑁෍
𝑖=1
𝑁
𝐿𝑖 𝐿 = 1
𝑁෍
𝑖=1
𝑁
𝐿𝑖 +λ𝑅(𝑊)
𝑅(𝑊) = ෍
𝑖
෍
𝑗
𝑊𝑖,𝑗
2
𝑅(𝑊) = ෍
𝑖
෍
𝑗
𝑊𝑖,𝑗
𝑅(𝑊) = ෍
𝑖
෍
𝑗
𝑊𝑖,𝑗
2 +𝛽 𝑊𝑖,𝑗 W(𝑡 +1) = 𝑊(𝑡)−η𝛻𝐽
W 𝑡 +1 = 𝑊 𝑡 −η𝛻𝐽+λ𝑅(𝑊)
Update without regularization
Update with regularization
Loss with regularizationLoss without regularization
![Page 21, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_021.png)

### Page 22
Drop-oupt
22
•  “drop-out”: randomly disconnect inputs from one layer to the next with a probability 𝑝: prob. to maintain
the link or to disconnect it (implementation dependent)
• For a batch, the links are deactivated for the forward and backward propagation stages, then reactivated and
randomly deactivated again for the next batch.
• dropout only is used in training.
•  Disconnecting connections randomly ensures that no single node is always responsible for "activating" when a
given pattern occurs. It ensures that there are multiple, redundant nodes that will be triggered when similar inputs
are presented, which in turn helps our model to generalise.
Without drop-out Drop-out with 𝑝 = 0.5
![Page 22, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_022.png)

### Page 23
23
KERAS
• Keras is a high-level framework for training neural networks. This library was
developed by François Chollet in 2015 with the aim of simplifying the programming
of algorithms based on deep learning.
• It offers a more intuitive and high-level set of abstractions. Training can still be
done on GPU, remembering that this is the only way we have to train a neural
network in an allowable time interval.
![Page 23, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_023.png)
![Page 23, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_023_img_005.png)
![Page 23, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_023_img_006.png)

### Page 24
24
Basic layers in Keras
keras.layers.Dense(units, activation=None, use_bias=True,
kernel_initializer='glorot_uniform', bias_initializer='zeros')
keras.layers.Activation(activation)
keras.layers.Dropout(rate, noise_shape=None, seed=None)
keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid')
keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid',
data_format=None)
keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,
center=True, scale=True)
keras.layers.Flatten(data_format=None)
https://keras.io/layers/core/
Keras documentation
![Page 24, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_024.png)

### Page 25
25
Architecture definition:
Sequential mode
Sequential mode (or API): An object of type Model () is instantiated and the layers
that make up the architecture are added one after the other.
# Imports
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
# Sequential model
model = Sequential()
# Architecture definition
model.add(Dense(64, input_dim=784))
model.add(Activation('relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))
https://keras.io/models/sequential/
![Page 25, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_025.png)

### Page 26
26
Architecture definition:
Functional mode
Functional mode (or API): An input is defined and the architecture is defined from these inputs (indicating which
is the input to each layer). Once the architecture has been defined, the model object is created by passing it the
inputs and outputs (last layer defined).
# Imports
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
# Input definition
inputs = Input(shape=(784,))
# Architecture definition
x = Dense(64, activation='relu')(inputs)
x = Dense(64, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)
# We create the model object as a union of inputs and architecture
model = Model(inputs=inputs, outputs=predictions)
https://keras.io/models/model/
![Page 26, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_026.png)

### Page 27
27
Compiling a Keras model
Before training the model in Keras, it is necessary to compile it by configuring the training
process. This process is carried out by means of the command model.compile.
# Example for a multi-class classification problem
model.compile(optimizer=‘sgd',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# Example for a binary classification problem
model.compile(optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9,
nesterov=True),
              loss='binary_crossentropy',
              metrics=['accuracy'])
# Example for a regression problem
model.compile(optimizer='rmsprop',
              loss='mse')
![Page 27, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_027.png)

### Page 28
28
Training in Keras
Once the model has been compiled in Keras it is possible to launch the training process. Keras trains models
from data and labels stored in Numpy arrays using the model.fit method.
# Data generation
import numpy as np
data = np.random.random((1000, 100))
labels = np.random.randint(2, size=(1000, 1))
# Example with labels in one-hot encoding
model.compile(optimizer='rmsprop',loss='categorical_crossentropy',
              metrics=['accuracy'])
one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)
model.fit(data, one_hot_labels, epochs=10, batch_size=32)
# Example with categorical labels
model.compile(optimizer='rmsprop', loss=‘sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(data, labels, epochs=10, validation_data=(data_v, labels_v),
batch_size=32)
![Page 28, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_028.png)

### Page 29
29
Prediction and evaluation in
Keras
Once the trained model is available, an evaluation of the model must be carried out. T o do
this, the test data is predicted with the model.predict command and then performance
metrics are obtained or model.evaluate is used
# Example with model.evaluate
model.evaluate(x=X_test, y=y_test, batch_size=None)
%% Returns the values of losses and metrics used in training for the test data.
# Example with model.predict and evaluation_report
from sklearn.metrics import classification_report
predictions = model.predict(x=X_test, batch_size=None)
print(classification_report(y_labels,predictions.argmax(axis=1))
%% Returns more metrics
![Page 29, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_029.png)

### Page 30
Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
30
![Page 30, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_030.png)

### Page 31
Concept of image
31
Gray image RGB image
level: 0….255 (black….white)
![Page 31, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_031.png)

### Page 32
Problem with images
32
28
28
784
200
200x784= 156.800 weights
Example: In the MNIST database for image classification
• When the size of the image starts to grow, using fully connected
layers (one neuron per pixel) becomes unfeasible.
![Page 32, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_032.png)
![Page 32, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_032_img_003.png)
![Page 32, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_032_img_005.png)
![Page 32, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_032_img_006.png)
![Page 32, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_032_img_007.png)

### Page 33
Hand-crafted learning
33
Hand-crafted learning
HOG
LBP
Co-occurrence
Granulometry SIFT
SURF
Colour
Data Feature Extraction Classification or
Regression
![Page 33, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_033.png)
![Page 33, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_033_img_010.png)
![Page 33, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_033_img_011.png)
![Page 33, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_033_img_012.png)

### Page 34
Automated learning
34
(Data)2
Feature extraction Classification
Deep neural networks
![Page 34, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_034.png)
![Page 34, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_034_img_009.png)

### Page 35
Convolutional neural networks
35
35
• Convolutional networks are a specialized kind of neural network for
processing data that has a grid-like topology. They take advantadge of:
– local connectivity
– parameter sharing
– pooling / subsampling hidden units
Multilayer Perceptron Convolutional neural network
3
![Page 35, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_035.png)
![Page 35, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_035_img_003.png)

### Page 36
Convolution
36
74 66 62 59 63 71 75 72
72 65 61 58 62 70 75 72
71 68 63 59 63 69 73 69
70 67 62 59 62 69 73 69
70 64 59 57 61 70 74 72
68 66 64 63 65 68 69 68
70 69 67 66 67 69 69 67
69 68 66 64 65 67 66 63
1 0 -1
2 0 -2
1 0 -1
-1 0 1
-2 0 2
-1 0 1
𝑦 𝑚,𝑛 = 𝑥 𝑚,𝑛 ∗ℎ 𝑚,𝑛 = ෍
𝑘
෍
𝑙
𝑥 𝑘,𝑙 ℎ[𝑚−𝑘,𝑛−𝑙]
Kernel
ℎ[−𝑘,−𝑙]ℎ[𝑘,𝑙]
−1 ∗62+0∗59+1∗62+(−2)∗59+0∗57+2∗61+(−1)∗64+0∗63+1∗65 = 3
3
Imagen de salida
![Page 36, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_036.png)

### Page 37
37
Convolution
1 0 -1
2 0 -2
1 0 -1
1 2 1
0 0 0
-1 -2 -1
![Page 37, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_037.png)

### Page 38
Local connectivity
38
38
 Receptive field
෍ 𝑓
 ෍ 𝑓
 ෍ 𝑓
… …
each hidden unit is connected only to a subregion (patch)
of the input image: receptive ﬁeld
w111 … w1N1
… … …
wN11 … wNN1
w111 … w1N1
… … …
wN11 … wNN1
![Page 38, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038.png)
![Page 38, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_003.png)
![Page 38, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_005.png)
![Page 38, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_006.png)
![Page 38, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_007.png)
![Page 38, Figure 6](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_008.png)
![Page 38, Figure 7](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_009.png)
![Page 38, Figure 8](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_010.png)
![Page 38, Figure 9](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_011.png)
![Page 38, Figure 10](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_012.png)
![Page 38, Figure 11](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_013.png)
![Page 38, Figure 12](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_038_img_015.png)

### Page 39
Parameter sharing
39
39
෍ 𝑓
 ෍ 𝑓
 ෍ 𝑓
… …
 ෍ 𝑓
 ෍ 𝑓
 ෍ 𝑓… …
 ෍ 𝑓
 ෍ 𝑓
 ෍ 𝑓… …
units organized into the same
‘‘feature map’’ share parameters
Feature map 1 Feature map 2 Feature map 3
hidden units within a feature
map cover diﬀerent positions in
the image
w111 … w1N1
… … …
wN11 … wNN1
w111 … w1N1
… … …
wN11 … wNN1
w111 … w1N1
… … …
wN11 … wNN1
Kernel of
feature
map 1
Kernel of
feature map 2
Kernel of
feature map 3
![Page 39, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039.png)
![Page 39, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_003.png)
![Page 39, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_004.png)
![Page 39, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_006.png)
![Page 39, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_008.png)
![Page 39, Figure 6](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_009.png)
![Page 39, Figure 7](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_010.png)
![Page 39, Figure 8](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_011.png)
![Page 39, Figure 9](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_012.png)
![Page 39, Figure 10](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_013.png)
![Page 39, Figure 11](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_014.png)
![Page 39, Figure 12](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_015.png)
![Page 39, Figure 13](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_016.png)
![Page 39, Figure 14](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_017.png)
![Page 39, Figure 15](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_018.png)
![Page 39, Figure 16](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_019.png)
![Page 39, Figure 17](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_020.png)
![Page 39, Figure 18](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_021.png)
![Page 39, Figure 19](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_022.png)
![Page 39, Figure 20](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_023.png)
![Page 39, Figure 21](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_024.png)
![Page 39, Figure 22](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_027.png)
![Page 39, Figure 23](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_028.png)
![Page 39, Figure 24](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_039_img_029.png)

### Page 40
Feature maps
40
Input image
Feature maps
Convolutions
w111 … w1N1
… … …
wN11 … wNN1
Learnable
![Page 40, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_040.png)
![Page 40, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_040_img_004.png)
![Page 40, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_040_img_005.png)
![Page 40, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_040_img_006.png)
![Page 40, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_040_img_007.png)

### Page 41
Feature maps
41
Input image Feature map 1 Feature map 2
![Page 41, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_041.png)

### Page 42
Pooling
42
maxpooling
Pooling is performed in non overlapping neighborhoods (subsampling)
![Page 42, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_042.png)
![Page 42, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_042_img_003.png)
![Page 42, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_042_img_005.png)
![Page 42, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_042_img_006.png)
![Page 42, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_042_img_007.png)
![Page 42, Figure 6](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_042_img_008.png)

### Page 43
Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
43
![Page 43, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_043.png)

### Page 44
Convolutional layers
44
Filtro
![Page 44, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_044.png)
![Page 44, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_044_img_003.png)

### Page 45
Convolutional layers
45
Presentación
N. of input images
𝑁𝑖𝑛𝑝𝑢𝑡
Image
size
Input images
(from the
previous layer)
Output images
(to the next layer)
Kernels
(𝐾 ×𝐾 ×𝑁𝑖𝑛𝑝𝑢𝑡 ×𝑁𝑜𝑢𝑡𝑝𝑢𝑡)
N. of output images
𝑁𝑜𝑢𝑝𝑢𝑡
𝐾
𝐾
𝑁𝑖𝑛𝑝𝑢𝑡
𝑁𝑜𝑢𝑡𝑝𝑢𝑡
![Page 45, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045.png)
![Page 45, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_003.png)
![Page 45, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_004.png)
![Page 45, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_005.png)
![Page 45, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_006.png)
![Page 45, Figure 6](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_007.png)
![Page 45, Figure 7](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_008.png)
![Page 45, Figure 8](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_009.png)
![Page 45, Figure 9](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_010.png)
![Page 45, Figure 10](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_011.png)
![Page 45, Figure 11](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_012.png)
![Page 45, Figure 12](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_013.png)
![Page 45, Figure 13](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_014.png)
![Page 45, Figure 14](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_045_img_015.png)

### Page 46
Convolutional layers: Stride
Input
image
7 x 7
Activation map
5 x 5
Stride = 1
Kernel = 3 x 3
• The center of the kernel (or mask filter) moves to the next pixel in steps given by
the Stride hyperparameter
Warning !!!
Reduction of the image size.
It depends on kernel size and stride46
![Page 46, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_046.png)

### Page 47
Convolutional layers: zero
padding
47
What happens when a 5 x 5 x 3 filter is applied to a 32 x 32 x 3 input volume (32 x 32
pixel RGB image)?
In the first layers of the network we want to preserve as much information as possible
from the original image.
Size of activation map: 28 x 28 x 3
Zero
Padding
5 x 5 x  3
Kernel
Ouptut image
28 x 28 x 3
Input image
32 x 32 x 3
Zero padding
36 x 36 x 3
Input image
32 x 32 x 3
5x5x3
Kernel
Ouptut image
28 x 28 x 3
![Page 47, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047.png)
![Page 47, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_003.png)
![Page 47, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_004.png)
![Page 47, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_005.png)
![Page 47, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_006.png)
![Page 47, Figure 6](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_007.png)
![Page 47, Figure 7](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_008.png)
![Page 47, Figure 8](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_009.png)
![Page 47, Figure 9](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_010.png)
![Page 47, Figure 10](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_011.png)
![Page 47, Figure 11](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_012.png)
![Page 47, Figure 12](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_013.png)
![Page 47, Figure 13](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_047_img_014.png)

### Page 48
Pooling layers
48
• This layer is a decimation layer in spatial dimension (width, height)
• Its function reduces the spatial size of the representation :
• Reduce the number of paremeters
• Reduce computational cost
• Avoid overfitting
Main functions:
- Max pooling
- Average pooling
- L2-norm pooling
Typical hyperparameters:
• Filter size = 2×2
• Stride = 2
Input volume Output volume
Stride=2
5 8
9 6
3 5
7 3
Avg.
![Page 48, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_048.png)
![Page 48, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_048_img_003.png)
![Page 48, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_048_img_004.png)
![Page 48, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_048_img_005.png)

### Page 49
CNN. Setup
• Among the different layers, some aditional parameters have to been
considered :
– Convolutional layer
• Kernel size
• Number of filters
• Stride
• Zero padding
– Pooling layer
• Stride
• Window size
– Activation layer
– Batch normalization layer
– Drop out layer
– Classification: Top model
49
![Page 49, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_049.png)

### Page 50
50
Basic layers in Keras
keras.layers.Dense(units, activation=None, use_bias=True,
kernel_initializer='glorot_uniform', bias_initializer='zeros')
keras.layers.Activation(activation)
keras.layers.Dropout(rate, noise_shape=None, seed=None)
keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid')
keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid',
data_format=None)
keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,
center=True, scale=True)
keras.layers.Flatten(data_format=None)
https://keras.io/layers/core/
Keras documentation
![Page 50, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_050.png)

### Page 51
Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
51
![Page 51, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_051.png)

### Page 52
CNN: Feature extraction
52
Feature visualization of convolutional net trained on ImageNet (Zeiler, Fergus, 2013)
• CNN = learning hierarchical representations with increasing levels of abstraction
• End to end training: joint optimization of features and classiﬁer
52
![Page 52, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_052.png)

### Page 53
53
Feature extraction
http://cs231n.stanford.edu/53
![Page 53, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_053.png)

### Page 54
Low-level features
54
• How does the network detect low-level features?
• Convolutional blocks: The output (activation
map) of the first convolutional layer will be the
input of the second layer,and so forth.
•The output of the second convolutional layer
will be the activations representing higher
level features: semicircles (combination of a
curve and a straight edge), squares
(combination of several straight edges), etc.
54
![Page 54, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_054.png)

### Page 55
High-level features
55
55
![Page 55, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_055.png)

### Page 56
High-level features
56
56
![Page 56, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_056.png)

### Page 57
High-level features
57
57 https://poloclub.github.io/cnn-explainer/
![Page 57, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_057.png)

### Page 58
Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
58
![Page 58, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_058.png)

### Page 59
CNN for classification
• Description of CNN
59
https://miro.medium.com/v2/resize:fit:847/1*eiWpS
Vh65QZN9usE6YRJTA.png
• Training: stochastic gradient descent/ forward-backward propagation
• Hyperparameters setting: as MLP
59
![Page 59, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_059.png)
![Page 59, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_059_img_004.png)

### Page 60
CNN for classification
60
Convolutional
layers
Feature extraction Classification
• Top model:
• Flatten layer
• Multilayer perceptrón
• The last fully connected layer is a softmax layer (in the case of more than one output
neuron) with as many neurons as the number of classes.
• PROBLEM: Overfitting
![Page 60, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_060.png)
![Page 60, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_060_img_003.png)
![Page 60, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_060_img_004.png)
![Page 60, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_060_img_005.png)
![Page 60, Figure 5](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_060_img_006.png)
![Page 60, Figure 6](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_060_img_007.png)
![Page 60, Figure 7](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_060_img_008.png)

### Page 61
CNN for classification
61
• Top model:
• Global averagepooling + softmax
• Global maxpooling+ softmax
Convolutional
layers
Feature extraction
Global average
pooling
Global max
pooling
Classification
GAP: calculates the average
output of each feature map
GMP: calculates the maximum
output of each feature map
![Page 61, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_061.png)
![Page 61, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_061_img_004.png)
![Page 61, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_061_img_007.png)

### Page 62
CNN.  Final Global Architecture
62
![Page 62, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_062.png)
![Page 62, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_062_img_003.png)

### Page 63
Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
63
![Page 63, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_063.png)

### Page 64
ImageNet
• ImageNet is a database containing 14 million images classified into 1000
different categories (http://www.image-net.org/).
• The most popular state-of-the-art architectures have been trained with ImageNet and
the resulting network weights are publicly available.
• The ImageNet Project runs an annual competition, the ImageNet Large Scale Visual
Recognition Challenge (ILSVRC), where algorithms compete to correctly classify
objects and scenes.
64
Objetive: to reduce training
parameters while maintaining or
exceeding the previous year
results, with a strong focus on
reducing computational cost.
![Page 64, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_064.png)
![Page 64, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_064_img_003.png)

### Page 65
LeNet-5
65
• LeNet-5 (1998, Lecun et al.): CNN pioneer.
• 7 layers
• Aplication: digit classification (banks to recognise
handwritten digits on cheques).
• Graylevel images of 28x28 pixels. Higher resolution
images: more convolutional layers.
Convolution Convolution
Convolution
Pooling
Pooling
Fully
Connected
![Page 65, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_065.png)

### Page 66
ALexNet
66
•  Winner of the 2012 ImageNet challenge and significantly outperformed all competitors by reducing the
top-5 error from 26% (runner-up) to 15.3%. SuperVision group.
•  Deeper than LeNet and with more filters per layer.
o 11x11, 5x5,3x3 convolutional filters and RELU activations.
o Max pooling layers
o Dropout
o Data augmentation
o SGD with momentum
o Trained for 6 days using 2 Nvidia Geforce GTX 580 GPUs.
![Page 66, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_066.png)
![Page 66, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_066_img_003.png)

### Page 67
VGG16 y VGG19
67
•  Fairly simple architecture: blocks composed of an incremental number of convolutional
layers with filters of size 3x3 with interleaved maxpooling layers (halving the size of the
activation maps).
• Classification block: 2 fully-connected layers (4096 neurons) and the output layer (1000
neurons).
• 16 and 19: differences in number of weighted layers in each network (convolutional and fully
connected).
Simonyan y Zisserman (2014). Very Deep
Convolutional Networks for Large Scale
Image recognition
(https://arxiv.org/abs/1409.1556)
![Page 67, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_067.png)
![Page 67, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_067_img_003.png)

### Page 68
Inception V3 (GoogleNet)
68
•  This type of architecture, introduced in 2014 by Szegedy et al. "Going Deeper with
Convolutions" (https://arxiv.org/abs/1409.4842), uses blocks with filters of different sizes that are
then concatenated in order to extract features at different scales that are then combined into a
single activation map.
• Challenge winners in 2014, top-5 error rate of 6.67%!
• This architecture, 22 convolutional layers, requires less memory than VGG and ResNet. It
reduces the number of parameters from 60 million (AlexNet) to 4 million.
![Page 68, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_068.png)

### Page 69
Xception
69
• Proposed by François Chollet (creator of Keras).  Like Inception but provides a quick way to
make 2D convolutions (2 1D convolutions).
"Xception: Deep Learning with Depthwise Separable Convolutions", https://arxiv.org/abs/1610.02357.
![Page 69, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_069.png)
![Page 69, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_069_img_003.png)

### Page 70
ResNet (Microsoft)
70
•  Developed by He et al. in 2015 ( 2015 winners): introduces an exotic type of
architecture based on modules (" networks within networks ").
• Introduces the concept of " residual connections ". In layer 𝑙+1 the activation
map of the unmodified and modified 𝑙-layer 𝑙+1 is combined..
“Deep Residual Learning for Image Recognition": https://arxiv.org/abs/1512.03385
![Page 70, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_070.png)
![Page 70, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_070_img_003.png)

### Page 71
ResNet
71
•  Improved architecture in 2016 improved by including more layers of residual blocks.
• There are variations of ResNet with different numbers of layers, but the most widely used is
ResNet50, which consists of 50 with weights.
• Many more layers than VGG but needs almost 5 times less memory: fully connected layers are
replaced by a type of layer called GlobalAveragePooling, which converts 2D activation maps
to a vector of n classes that is used to calculate the probability of belonging to each class.
![Page 71, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_071.png)
![Page 71, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_071_img_003.png)

### Page 72
Comparison of sizes
72
Source: AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS, 2017
![Page 72, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_072.png)
![Page 72, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_072_img_003.png)

### Page 73
73
Comparison: accuracy vs number of
parameters
Source: AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS, 2017
![Page 73, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_073.png)
![Page 73, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_073_img_003.png)

### Page 74
Transfer learning
•  Training a neural network is costly (time).
• Transfer learning techniques to avoid:
o Define the architecture of a neural network.
o Train it from the beginning.
• Idea: Initialise the weights of a predefined network with values that classify a given dataset
well and tune them to our problem.
• We avoid:
o Need for a dataset as large as necessary if we want to train a network from scratch (from hundreds
of thousands or even millions of images we could go to a few thousand).
o Need to wait a good number of epochs to get values for the optimal weights for classification.
•  Two techniques:
– Transfer learning
– Fine-tuning
74
![Page 74, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_074.png)

### Page 75
Transfer Learning vs Fine-
Tuning
75
![Page 75, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_075.png)

### Page 76
Transfer Learning
76
• SVM
• Random Forest
• etc.
Extractor de caracterísiticas Clasificado
r
![Page 76, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_076.png)

### Page 77
Fine-tuning
77
Shallow-tuning
![Page 77, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_077.png)

### Page 78
Fine-tuning
78
Deep-tuning
![Page 78, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_078.png)

### Page 79
Fine-tuning
79
Deep-tuning
![Page 79, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_079.png)

### Page 80
Fine-tuning
80
From
scratch
![Page 80, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_080.png)

### Page 81
Practical Protocol
81
In practise:
1. Modify only the last layer to have the same number of outputs as
classes(baseline).
2. Modify the top model and re-train the classifying stage (FC
layers) (“shallow tuning”).
3. Re-train convolutional blocks (“deep tuning”). In each iteration
one more, starting from the output.
Tips depending on our dataset
o If it is small and similar to the original: transfer learning (SVM for
example). It helps to prevent overfitting.
o If it is large and similar to the original: as we have more data we
probably won't incur in over-fitting, so we can do fine-tuning with
more confidence.
![Page 81, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_081.png)

### Page 82
Practical protocol
82
Tips depending on our dataset
o If it is small and very different from the original: transfer learning
but with characteristics of layers before the last convolutional.
o If it is large and very different from the original: train the network
from scratch. (from scratch).
Note the possible restrictions of pre-trained
models. For example, they may require a
minimum image size. In addition, when re-
training networks, learning rates are usually
chosen lower than if we do it from scratch,
since we start from an initialisation of
weights that is assumed to be good.
![Page 82, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_082.png)

### Page 83
Contents
1. Revisiting the MLP
2. Introduction to CNNs
3. CNN layers
4. Feature extraction
5. CNN architecture for classification
6. Transfer Learning
7. Practical aspects
83
![Page 83, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_083.png)

### Page 84
84
Image DataGenerator class
• Keras has implemented functionalities to facilitate/optimise data loading.
• It is possible to load the training data in RAM in batches (batch by batch) to perform the steps
that make up a training step and not having to store the entire dataset in memory.
• ImageDataGenerator is nothing more than an object type that can apply or not certain
transformations to the data being loaded by means of a series of methods of this object that
facilitate the task.
• The ImageDataGenerator object does not store the data itself.
• The transformations that ImageDataGenerator allows serve to preprocess the data, rescale it or
establish a validation partition, but its main function is that it implements the functionality to
create synthetic image samples.
https://keras.io/api/preprocessing/image/#imagedatagenerator-class
![Page 84, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_084.png)

### Page 85
85
Loading batches of data from
disk
The ImageDataGenerator object includes a series of methods that facilitate the task of loading data
from disk. These methods allow the data to be loaded gradually into memory (batch by batch)
during the training process.
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator
![Page 85, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_085.png)
![Page 85, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_085_img_003.png)

### Page 86
86
Loading batches of data
from disk
The ImageDataGenerator object includes a series of methods that facilitate the task of loading data
from disk. These methods allow the data to be loaded gradually into memory (batch by batch)
during the training process.
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator
![Page 86, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_086.png)
![Page 86, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_086_img_003.png)
![Page 86, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_086_img_004.png)

### Page 87
87
•  Objetive: Increase the number of images in the training set to combat overfitting.
•  How?
o Rotation,
o Re-scale,
o Translation,
o Zoom,
o Flips (Horizontal and Vertical),
o Changes in color.
• This is done in real time, during training. It is not necessary to save the augmented
images.
• The new images take the class of the source image, so the transformations cannot
be too exaggerated, let alone cause them to resemble another class.
Data Augmentation
![Page 87, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_087.png)

### Page 88
Early Stopping
88
https://keras.io/api/callbacks/early_stopping/
![Page 88, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_088.png)
![Page 88, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_088_img_004.png)

### Page 89
89
Data augmentation
# Creating an instance of ImageDataGenerator class
train_datagen = ImageDataGenerator(rescale=1. / 255,
                 rotation_range = 0.2,
                                   zoom_range = 0.2,
        horizontal_flip = True,
                                   vertical_flip = True)
# Creating an instance of ImageDataGenerator class
validation_generator = validation_datagen.flow_from_directory(validationDirectory,
                                                      target_size=(img_width, img_height),
                                                      batch_size=batch_size,
                                                      class_mode='categorical’)
Source: https://medium.com/towards-data-science/image-augmentation-for-deep-learning-
using-keras-and-histogram-equalization-9329f6ae5085 )
![Page 89, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_089.png)

### Page 90
Reduce learning rate
90
https://keras.io/api/callbacks/learning_rate_scheduler/
Step decay
Exponential decay
Cosine Annealing
![Page 90, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_090.png)
![Page 90, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_090_img_004.png)
![Page 90, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_090_img_005.png)
![Page 90, Figure 4](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_090_img_006.png)

### Page 91
Model checkpoint
91
https://keras.io/api/callbacks/model_checkpoint/
![Page 91, Figure 1](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_091.png)
![Page 91, Figure 2](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_091_img_003.png)
![Page 91, Figure 3](subjects/Admeav/Teoria/slides/T2_CNN based feature extraction _1_-images/T2_CNN based feature extraction _1__page_091_img_004.png)
