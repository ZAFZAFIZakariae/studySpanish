# Capítulo 4 · Representaciones de texto y embeddings

## Objetivos
- Conocer la evolución desde representaciones discretas hasta embeddings continuos.
- Revisar técnicas clásicas (One-Hot, Bag-of-Words, TF-IDF) y sus limitaciones.
- Comprender modelos distribuidos como Word2Vec, GloVe y fastText.

## Representaciones discretas
- **One-Hot encoding:** vectores binarios, fáciles de implementar pero muy dispersos y sin noción de similitud.
- **Bag-of-Words:** cuenta ocurrencias de palabras; ignora orden y contexto.
- **TF-IDF:** pondera frecuencia local y relevancia global para resaltar términos distintivos.

## Limitaciones
- Alta dimensionalidad, matrices dispersas y ausencia de relaciones semánticas.
- No generalizan a palabras fuera de vocabulario ni capturan polisemia.

![Transición de representaciones](figure:snlp-chapter-4/representaciones)
Caption: De vectores dispersos a embeddings densos que preservan similitudes semánticas.

## Embeddings distribuidos
- **Word2Vec:** aprende vectores densos prediciendo contexto (Skip-gram) o palabra central (CBOW); preserva relaciones lineales.
- **GloVe:** factoriza coocurrencias globales para capturar semántica y analogías.
- **fastText:** incorpora subpalabras para generalizar a palabras raras o morfológicamente complejas.

## Propiedades
- Vectores densos de baja dimensión (50–300) que permiten medir similitud coseno.
- Capturan regularidades semánticas y sintácticas (ej. rey − hombre + mujer ≈ reina).
- Sirven como inicialización para modelos más profundos o como características directas.

## Ejemplos prácticos
- Uso de bibliotecas (`gensim`, `fasttext`, `spacy`) para entrenar o cargar embeddings preentrenados.
- Visualización con PCA/t-SNE para explorar relaciones entre palabras.
- Aplicaciones: clasificación de texto, recuperación de información, chatbots, generación.

## Hacia modelos contextualizados
- Limitaciones de embeddings estáticos frente a polisemia.
- Introducción a modelos contextualizados (ELMo, BERT) que adaptan vectores según el contexto.
- Anticipa el estudio de Transformers y modelos de lenguaje masivos.
