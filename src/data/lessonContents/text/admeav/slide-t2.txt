# T2 · CNN-Based Feature Extraction (Extracción de características con CNN)

## Purpose / Propósito
- Connect dense networks with their convolutional extensions to understand how hierarchical features emerge. / Conectar las redes densas con su extensión convolucional para entender cómo surgen características jerárquicas.
- Analyze architectures and practices that allow training robust, reusable CNNs. / Analizar arquitecturas y prácticas que permiten entrenar CNN robustas y reutilizables.

## Objectives / Objetivos
- Revisit multilayer perceptrons and backpropagation as the starting point. / Recordar perceptrones multicapa y retropropagación como punto de partida.
- Understand the role of convolutions, pooling, normalization, and activations in feature extraction. / Comprender el rol de convoluciones, pooling, normalización y activaciones en la extracción de características.
- Review reference architectures and transfer learning strategies. / Revisar arquitecturas de referencia y estrategias de transferencia de aprendizaje.
- Identify good training practices to avoid overfitting and improve generalization. / Identificar buenas prácticas de entrenamiento para evitar sobreajuste y mejorar generalización.

## Activities / Actividades
### Session Outline (Itinerario)
1. Review MLPs and backpropagation. / Repaso de MLP y retropropagación.
2. Introduce convolutions and activation maps. / Introducción a convoluciones y mapas de activación.
3. Cover fundamental layers: convolution, pooling, normalization, activations. / Capas fundamentales: convolución, pooling, normalización, activaciones.
4. Build feature-extraction pipelines. / Construcción de pipelines de extracción de características.
5. Discuss reference architectures (LeNet, VGG, ResNet). / Arquitecturas de referencia (LeNet, VGG, ResNet).
6. Explore transfer learning and fine-tuning. / Transfer learning y ajuste fino.
7. Summarize good training practices. / Buenas prácticas de entrenamiento.

### Concept Walkthrough (Recorrido conceptual)
- From MLPs to convolutions: shared weights and local connectivity reduce parameters while preserving spatial structure. / Del MLP a la convolución: pesos compartidos y conectividad local reducen parámetros y preservan estructura espacial.
- Key layers / Capas clave:
  - **Convolution:** kernel, stride, and padding define the activation map; it learns edge, texture, or part detectors. / **Convolución:** kernel, stride y padding determinan el mapa de activación; aprende detectores de bordes, texturas o partes.
  - **Pooling:** lowers resolution and adds invariance; can be replaced by convolutions with stride > 1. / **Pooling:** reduce resolución y aporta invariancia; puede sustituirse por convoluciones con stride > 1.
  - **Normalization:** batch/layer norm stabilizes training. / **Normalización:** batch/layer norm estabiliza el entrenamiento.
  - **Activations:** ReLU, GELU, and variants keep gradients useful. / **Activaciones:** ReLU, GELU y variantes mantienen gradientes útiles.
- Extraction pipeline / Pipeline de extracción:
  - Early layers → edges and colors. / Capas iniciales → bordes y colores.
  - Middle layers → textures and parts. / Capas intermedias → texturas y partes.
  - Final layers → global representations reusable in downstream tasks. / Capas finales → representaciones globales reutilizables en tareas downstream.
- Architectures for classification / Arquitecturas para clasificación:
  - **LeNet-5** as a pioneer. / **LeNet-5** como pionera.
  - **VGG** with stacked 3×3 filters and increasing depth. / **VGG** con pilas 3×3 y profundidad creciente.
  - **ResNet** with residual shortcuts; extensions include Inception, DenseNet, and hybrid attention. / **ResNet** con atajos residuales; extensiones con Inception, DenseNet y atención híbrida.
- Transfer learning:
  - Freeze early layers and adapt final ones with limited data. / Congelar capas iniciales y ajustar finales con pocos datos.
  - Fine-tune the entire network with a low learning rate when the domain changes. / Fine-tuning completo con learning rate bajo cuando cambia el dominio.
  - Use pretrained features as input to classical classifiers. / Usar features preentrenadas como entrada a clasificadores clásicos.
- Good practices / Buenas prácticas:
  - Data augmentation (flip, crop, color jitter). / Data augmentation (flip, crop, color jitter).
  - Optimizers: SGD + momentum, Adam, RMSprop. / Optimizadores: SGD + momentum, Adam, RMSprop.
  - Tune learning rate, L2 regularization, dropout, and early stopping. / Ajuste de LR, regularización L2, dropout y early stopping.
- ![Feature hierarchy in a CNN / Jerarquía de características en una CNN](figure:admeav/t2-hierarchy)
  Caption: From simple edges to high-level concepts as layers deepen. / De bordes simples a conceptos de alto nivel conforme avanzan las capas.

### Further Reading (Lecturas recomendadas)
- *Deep Learning* (Ch. 9) — Goodfellow et al. / *Deep Learning* (Cap. 9) — Goodfellow et al.
- PyTorch/TensorFlow tutorials on transfer learning. / Tutoriales de PyTorch/TensorFlow sobre transferencia de aprendizaje.

## Assessment / Evaluación
- Solve short exercises that identify each layer's role in a CNN diagram. / Resolver ejercicios cortos que identifiquen la función de cada capa en un diagrama CNN.
- Prepare a comparative table between two architectures (e.g., VGG vs. ResNet) highlighting depth, parameters, and use cases. / Preparar una tabla comparativa entre dos arquitecturas (p.ej., VGG vs. ResNet) destacando profundidad, parámetros y casos de uso.
- Implement a brief transfer-learning experiment and report validation metrics. / Implementar un experimento breve de transfer learning y reportar métricas de validación.
