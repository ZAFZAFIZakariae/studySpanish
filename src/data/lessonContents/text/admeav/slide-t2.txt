# T2 · CNN-Based Feature Extraction

## Objetivos de la lección
- Recordar la base de las redes neuronales densas para entender su extensión convolucional.
- Comprender cómo capas convolucionales, de pooling y normalización extraen características jerárquicas.
- Analizar arquitecturas CNN para clasificación y estrategias de transferencia.
- Revisar prácticas recomendadas para entrenar modelos robustos.

## Itinerario
1. Repaso de perceptrones multicapa (MLP) y retropropagación.
2. Introducción a convoluciones y mapas de activación.
3. Capas fundamentales: convolución, pooling, normalización y activaciones.
4. Construcción de pipelines de extracción de características.
5. Arquitecturas de referencia (LeNet, VGG, ResNet).
6. Transfer learning y ajuste fino.
7. Consideraciones prácticas de entrenamiento.

![Jerarquía de características en una CNN](figure:admeav/t2-hierarchy)
Caption: Evolución de las representaciones desde bordes simples hasta conceptos de alto nivel a medida que avanzan las capas.

## Del MLP a la convolución
- Un MLP conecta completamente cada capa, lo que genera millones de parámetros y pierde la estructura espacial.
- Las CNN imponen **peso compartido** y **conectividad local**:
  - Filtros 2D recorren la imagen generando mapas de activación.
  - La misma máscara detecta patrones independientemente de la posición.
- Permiten capturar translaciones y patrones repetitivos con menos parámetros.

## Capas clave
- **Convolución:** definida por tamaño de kernel, stride y padding. Cada filtro aprende detectores de bordes, texturas o partes.
- **Pooling:** reduce resolución y aporta invariancia (max, average). Puede reemplazarse por convoluciones con stride > 1.
- **Normalización:** batch/layer norm estabiliza el entrenamiento y acelera la convergencia.
- **Activaciones no lineales:** ReLU, GELU y variantes evitan saturación y facilitan gradientes.

## Pipeline de extracción
- Primeras capas capturan **bajas frecuencias** (bordes, colores). Capas intermedias combinan patrones (texturas, partes).
- Las últimas capas densas sintetizan representaciones globales para clasificación.
- Es común usar salidas intermedias como **descriptores aprendidos** para tareas downstream (retrieval, detección).

## Arquitecturas para clasificación
- **LeNet-5:** pionera para dígitos; muestra combinación convolución-pooling.
- **VGG:** pilas de convoluciones 3×3 con profundidad creciente, trade-off entre precisión y coste.
- **ResNet:** bloques residuales que mitigan el desvanecimiento del gradiente y permiten redes muy profundas.
- Estrategias modernas incluyen **inception**, **dense connections** y **attention** híbrida.

## Transfer learning
- Congelar capas iniciales y ajustar las finales con pocos datos etiquetados.
- Re-entrenar (fine-tuning) todo el modelo con tasa de aprendizaje baja cuando el dominio objetivo difiere significativamente.
- Usar **features preentrenadas** como entrada de clasificadores clásicos (SVM, Random Forest) para prototipos rápidos.

## Buenas prácticas de entrenamiento
- Data augmentation (flip, crop, color jitter) para mejorar generalización.
- Selección de optimizadores: SGD con momentum, Adam, RMSprop según estabilidad deseada.
- Ajuste de hiperparámetros: tasa de aprendizaje, regularización L2, dropout.
- Monitorizar métricas de validación y aplicar early stopping para evitar sobreajuste.

## Lecturas recomendadas
- *Deep Learning* (Cap. 9) — Goodfellow et al.
- Tutoriales de PyTorch/TensorFlow sobre transferencia de aprendizaje.
