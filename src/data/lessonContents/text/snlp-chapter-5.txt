# Capítulo 5 · Transformers y modelos de lenguaje

## Contexto histórico
- Modelos basados en conteo (Bag-of-Words, TF-IDF) ignoran orden y contexto.
- Embeddings estáticos (Word2Vec, GloVe) capturan semántica pero no desambiguación contextual.
- RNN/LSTM procesan secuencias pero sufren gradientes vanishing/exploding y dificultad con dependencias largas.

## Limitaciones de RNN/LSTM
- Procesamiento estrictamente secuencial que impide paralelización.
- Memoria limitada para relaciones lejanas.
- Uso de embeddings estáticos y contexto unidireccional.

![Evolución hacia Transformers](figure:snlp-chapter-5/evolucion)
Caption: De modelos secuenciales con memoria limitada a arquitecturas basadas en atención.

## Nacimiento del Transformer
- Artículo “Attention is All You Need” introduce una arquitectura basada exclusivamente en atención.
- Permite procesar secuencias en paralelo, capturar dependencias largas y usar codificaciones posicionales.
- Componentes clave: multi-head self-attention, capas feed-forward, normalización y residual connections.

## Atención y multi-head
- La atención calcula pesos que determinan qué tokens son relevantes entre sí.
- Múltiples cabezas exploran diferentes subespacios de representación.
- `Scaled Dot-Product Attention` es el núcleo de las operaciones.

## Arquitecturas basadas en Transformer
- **BERT:** encoder bidireccional preentrenado con Masked LM; excelente en comprensión.
- **GPT:** decoder autoregresivo enfocado en generación; variantes GPT-2/3/4 escalan parámetros.
- **BART/T5:** combinan encoder-decoder para tareas de secuencia a secuencia; entrenamiento con objetivos de reconstrucción.

## Modelos de lenguaje grandes (LLM)
- Escalan parámetros (billones), datos y compute; aprenden capacidades emergentes.
- Se afinan con *instruction tuning* y *RLHF* para alinearse con necesidades humanas.
- Aplicaciones: asistentes conversacionales, generación de código, análisis semántico avanzado.

## ¿Por qué funcionan tan bien?
- Paralelismo masivo permite entrenar con grandes corpus.
- Atención captura contexto global sin perder detalles locales.
- Preentrenamiento + fine-tuning/adaptación rápida permiten reutilizar conocimiento.

## Consideraciones éticas y operativas
- Riesgos: sesgos, alucinaciones, consumo energético, privacidad.
- Necesidad de monitorizar respuestas, aplicar filtros y documentar limitaciones.
- Evaluar métricas de seguridad, factualidad y responsabilidad social.
